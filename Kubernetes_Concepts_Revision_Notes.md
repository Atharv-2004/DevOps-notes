# Kubernetes Concepts – Class Revision Notes  
**Date:** 8 December 2025

Kubernetes is an open-source platform used to automate the deployment, scaling, and operation of containerized applications. It enables efficient management of applications across a cluster of machines by abstracting infrastructure complexity and providing built-in mechanisms for resilience and scalability.

A Pod is the fundamental unit of execution in Kubernetes. It represents a running process within the cluster and can contain one or more containers. All containers inside a Pod share the same network namespace, including a single IP address and port space, which allows them to communicate locally as if they were running on the same machine. Pods are co-located and co-started, meaning all containers within a Pod are scheduled on the same node and managed together. A useful analogy is that a Pod is like a house, while containers are rooms inside it—the address belongs to the house, not the individual rooms.

A ReplicaSet is responsible for ensuring that a specified number of Pod replicas are running at any given time. If a Pod fails or is terminated, the ReplicaSet automatically creates a new Pod to replace it, maintaining application availability. ReplicaSets support both scaling out and scaling in. Even when only a single Pod is required, using a ReplicaSet is recommended because it provides fault tolerance by recreating the Pod if it crashes.

Deployments build on top of ReplicaSets and Pods by providing a declarative way to manage application updates. A Deployment defines the desired state for Pods and ReplicaSets and manages transitions between states. Deployments make it easy to scale applications up or down and support rolling updates, allowing new versions of an application to be deployed gradually without downtime. They also enable rollbacks, making it possible to revert to a previous stable version if issues arise. Internally, a Deployment manages ReplicaSets, which in turn manage Pods.

Services in Kubernetes provide a stable network endpoint for accessing applications running on Pods. Since Pods are ephemeral and their IP addresses can change, Services abstract this complexity by offering a consistent way to reach applications. A ClusterIP Service exposes an application on an internal IP address that is only accessible within the cluster. A NodePort Service exposes the application on a static port across each node’s IP address and automatically creates an associated ClusterIP Service. Services rely on labels for service discovery and distribute incoming traffic evenly across all matching Pods, effectively providing load balancing. An analogy is a movie theater with multiple attendants—customers can approach any attendant, and service is handled smoothly without congestion at a single point.

Load balancers are used to distribute incoming network traffic across multiple Pods. Internal load balancers operate within the cluster to balance traffic between Pods, while external load balancers handle traffic coming from outside the cluster, such as internet-facing requests routed to Kubernetes services.

Kubernetes resources are defined using YAML configuration files. These files follow a common structure that includes the API version, which specifies the Kubernetes API being used; the kind, which defines the type of object such as a Pod, Deployment, or Service; metadata, which contains identifying information like the resource name and optional namespace; and spec, which describes the desired behavior and configuration of the object.

Together, these concepts—Pods, ReplicaSets, Deployments, Services, load balancers, and YAML-based configurations—form the foundation of Kubernetes. Understanding how they interact enables effective deployment, scaling, and management of containerized applications in modern cloud-native environments.
